{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： /storage/user/huju/transferred/ws_lighthouse/lighthouse\n",
      "改变后的工作目录： /storage/user/huju/transferred/ws_lighthouse\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "# 打印当前工作目录\n",
    "print(\"当前工作目录：\", os.getcwd())\n",
    "# 改变工作目录\n",
    "new_directory = \"/storage/user/lhao/hjp/ws_lightshouse\"\n",
    "os.chdir(new_directory)\n",
    "# 再次打印当前工作目录，确认改变\n",
    "print(\"改变后的工作目录：\", os.getcwd())\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lighthouse.geometry.projector as pj\n",
    "from lighthouse.mlv import MLV\n",
    "import lighthouse.nets as nets\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lighthouse.data_loader import MyDataloader_lzq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-842ff5402732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mmbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mbatch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref_image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# = 6 input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ref_depth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# = 6 input depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intrinsics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "mykey = lambda x:int(x.split(\".\")[0].split(\"_\")[1])\n",
    "\n",
    "def pose2H(pose):\n",
    "    t = pose[-3:]\n",
    "    q = pose[:4]\n",
    "    q = q[[1,2,3,0]]\n",
    "    Rot = R.from_quat(q).as_matrix()\n",
    "    H = np.eye(4)\n",
    "    H[:3,:3] = Rot\n",
    "    H[:3,3] = t\n",
    "    return H\n",
    "\n",
    "def get_mbatch(id_ref=1,id_src=3,id_env=99):\n",
    "\n",
    "    mbatch = {}\n",
    "\n",
    "    mimage_root = \"/home/wiss/lhao/junpeng/ws_lighthouse/data/setup_ptc/scene0370_02/pfm\"\n",
    "    mimage_dirs = sorted(os.listdir(mimage_root),key=mykey)\n",
    "\n",
    "    # id_ref = 1\n",
    "    mref_image_path = os.path.join(mimage_root,mimage_dirs[id_ref])\n",
    "    # print(mref_image_path)\n",
    "\n",
    "    mdepth_root = mimage_root.replace(\"pfm\",\"depth\")\n",
    "    # mdepth_path = \"/home/wiss/lhao/junpeng/ws_lighthouse/data/setup_ptc/scene0370_02/depth/bgrdepth_2.png\"\n",
    "    mdepth_path = os.path.join(mdepth_root,sorted(os.listdir(mdepth_root),key=mykey)[id_ref])\n",
    "    \n",
    "    mintrinsic = np.array([[577.8705679012345,0,320],[0,577.8705679012345,240],[0, 0, 1]])\n",
    "\n",
    "    # ref_pose\n",
    "    raw = np.loadtxt(\"/home/wiss/lhao/junpeng/ws_lighthouse/data/setup_ptc/scene0370_02/cam_poses0.txt\")\n",
    "    mref_pose = raw[id_ref,:]\n",
    "    mt = mref_pose[-3:]\n",
    "    mq = mref_pose[:4]\n",
    "    mq = mq[[1,2,3,0]]\n",
    "    mRot = R.from_quat(mq).as_matrix()\n",
    "    mH = np.eye(4)\n",
    "    mH[:3,:3] = mRot\n",
    "    mH[:3,3] = mt\n",
    "\n",
    "    # src_images\n",
    "    # id_src = 3\n",
    "    msrc_image_path = os.path.join(mimage_root,mimage_dirs[id_src])\n",
    "\n",
    "    # src_pose\n",
    "    msrc_pose = raw[id_src,:]\n",
    "    mH_src = pose2H(msrc_pose)\n",
    "\n",
    "    # env_pose\n",
    "    # id_env = 99\n",
    "    raw_env = np.loadtxt(\"/storage/user/lhao/hjp/ws_superpixel/output/test2_300/2frame0370_02/2frame0370_02_control_cam_pose.txt\",skiprows=0)\n",
    "    menv_pose = raw_env[id_env]\n",
    "    mH_env = pose2H(menv_pose)[np.newaxis,...]\n",
    "\n",
    "    mbatch[\"ref_image\"] = cv2.imread(mref_image_path, cv2.IMREAD_UNCHANGED)[np.newaxis, ...]\n",
    "    mbatch[\"ref_depth\"] = cv2.imread(mdepth_path,cv2.CV_16UC1)[np.newaxis, ...]/5000.0\n",
    "    mbatch[\"intrinsics\"] = mintrinsic[np.newaxis, ...]\n",
    "    mbatch[\"ref_pose\"] = mH[np.newaxis, ...]\n",
    "    mbatch[\"src_images\"] = cv2.imread(msrc_image_path, cv2.IMREAD_UNCHANGED)[np.newaxis, ...]\n",
    "    mbatch[\"src_poses\"] = mH_src[np.newaxis,...,np.newaxis]\n",
    "    mbatch[\"env_pose\"] = mH_env.astype(np.float32)\n",
    "\n",
    "    return mbatch\n",
    "    \n",
    "def get_mbatch2(id_ref=1,id_src=3,id_env=1):\n",
    "\n",
    "    mbatch = {}\n",
    "    \n",
    "    mimage_root = \"/storage/user/lhao/hjp/ws_lightshouse/lighthouse_dataset/scene0370_02/ldr\"\n",
    "    mimage_dirs = sorted(os.listdir(mimage_root),key=mykey)\n",
    "\n",
    "    mref_image_path = os.path.join(mimage_root,mimage_dirs[id_ref])\n",
    "\n",
    "    mdepth_root = mimage_root.replace(\"ldr\",\"depth\")\n",
    "    mdepth_path = os.path.join(mdepth_root,sorted(os.listdir(mdepth_root),key=mykey)[id_ref])\n",
    "    \n",
    "    mintrinsic = np.array([[577.8705679012345,0,320],[0,577.8705679012345,240],[0, 0, 1]])\n",
    "\n",
    "    # ref_pose\n",
    "    raw = np.loadtxt(\"/home/wiss/lhao/junpeng/ws_lighthouse/data/setup_ptc/scene0370_02/cam_poses0.txt\")\n",
    "    mref_pose = raw[id_ref,:]\n",
    "    mt = mref_pose[-3:]\n",
    "    mq = mref_pose[:4]\n",
    "    mq = mq[[1,2,3,0]]\n",
    "    mRot = R.from_quat(mq).as_matrix()\n",
    "    mH = np.eye(4)\n",
    "    mH[:3,:3] = mRot\n",
    "    mH[:3,3] = mt\n",
    "\n",
    "    # src_images\n",
    "    # id_src = 3\n",
    "    msrc_image_path = os.path.join(mimage_root,mimage_dirs[id_src])\n",
    "\n",
    "    # src_pose\n",
    "    msrc_pose = raw[id_src,:]\n",
    "    mH_src = pose2H(msrc_pose)\n",
    "\n",
    "    # env_pose\n",
    "    # id_env = 99\n",
    "    raw_env = np.loadtxt(\"/storage/user/lhao/hjp/ws_superpixel/output/test2_300/2frame0370_02/2frame0370_02_control_cam_pose.txt\",skiprows=0)\n",
    "    menv_pose = raw_env[id_env]\n",
    "    mH_env = pose2H(menv_pose)[np.newaxis,...]\n",
    "\n",
    "    mbatch[\"ref_image\"] = cv2.imread(mref_image_path, cv2.IMREAD_UNCHANGED)[np.newaxis, ...]\n",
    "    mbatch[\"ref_depth\"] = cv2.imread(mdepth_path,cv2.CV_16UC1)[np.newaxis, ...]/5000.0\n",
    "    mbatch[\"intrinsics\"] = mintrinsic[np.newaxis, ...]\n",
    "    mbatch[\"ref_pose\"] = mH[np.newaxis, ...]\n",
    "    mbatch[\"src_images\"] = cv2.imread(msrc_image_path, cv2.IMREAD_UNCHANGED)[np.newaxis, ...]\n",
    "    mbatch[\"src_poses\"] = mH_src[np.newaxis,...,np.newaxis]\n",
    "    mbatch[\"env_pose\"] = mH_env.astype(np.float32)\n",
    "\n",
    "\n",
    "mbatch = get_mbatch2()\n",
    "print(mbatch[\"ref_image\"].shape) # = 6 input image\n",
    "print(mbatch[\"ref_depth\"].shape) # = 6 input depth\n",
    "print(mbatch[\"intrinsics\"].shape)\n",
    "print(mbatch[\"src_images\"].shape)\n",
    "print(mbatch[\"ref_pose\"].shape)\n",
    "print(mbatch[\"src_poses\"].shape)\n",
    "print(mbatch[\"env_pose\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================using a training dataset of length: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['ref_image', 'ref_depth', 'env_image', 'intrinsics', 'ref_pose', 'env_pose'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"/storage/user/lhao/hjp/ws_interiornet_stable/output/setup_ptc_ours\"\n",
    "start_end=(0.8,1)\n",
    "\n",
    "mydataloader = MyDataloader_lzq(root_dir=root_dir,start_end=start_end)\n",
    "\n",
    "ourbatch = mydataloader[5]\n",
    "ourbatch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object MyDataloader_lzq.tf_data_generator at 0x7f5673e1b1a8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iterator = mydataloader.tf_data_generator()\n",
    "train_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_generator() got an unexpected keyword argument 'output_signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-884d87b04153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: from_generator() got an unexpected keyword argument 'output_signature'"
     ]
    }
   ],
   "source": [
    "output_signature = {\n",
    "    \"ref_image\":tf.placeholder(dtype=tf.float32, shape=[None, height, width, 3]),\n",
    "    \"ref_depth\":tf.placeholder(dtype=tf.float32, shape=[None, height, width]),\n",
    "    \"intrinsics\":tf.placeholder(dtype=tf.float32, shape=[None, 3, 3]),\n",
    "    \"src_poses\":tf.placeholder(dtype=tf.float32, shape=[None, 4, 4, 1]),\n",
    "    \"env_pose\":tf.placeholder(dtype=tf.float32, shape=[None, 4, 4])\n",
    "}\n",
    "\n",
    "\n",
    "tf.data.Dataset.from_generator(mydataloader,output_signature=output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"./testset/test_set_with_coords_fixed/\"\n",
    "# checkpoint_dir = \"./lighthouse/model/\"\n",
    "checkpoint_dir = \"/storage/user/lhao/hjp/ws_lightshouse/lighthouse/experiment_finetune\"\n",
    "# output_dir = \"./lighthouse_output/\"\n",
    "output_dir = \"/home/wiss/lhao/junpeng/ws_lighthouse/output_lzq2frame_our\"\n",
    "\n",
    "batch_size = 1  # implementation only works for batch size 1 currently.\n",
    "height = 480  # px\n",
    "width = 640  # px\n",
    "env_height = 256  # px\n",
    "env_width = 512  # px\n",
    "cube_res = 64  # px\n",
    "theta_res = 512  # px\n",
    "phi_res = 256  # px\n",
    "r_res = 128  # px\n",
    "scale_factors = [2, 4, 8, 16]\n",
    "num_planes = 32\n",
    "depth_clip = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "True\n",
      "1.15.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n",
      "True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.gpu_device_name())\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.__version__)\n",
    "print(tf.config.experimental.list_physical_devices(device_type=\"GPU\"))\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/mlv.py:657: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/geometry/projector.py:494: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/geometry/sampling.py:162: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/geometry/sampling.py:178: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/nets.py:29: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/nets.py:97: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv3D` instead.\n",
      "WARNING:tensorflow:From /usr/wiss/lhao/anaconda3/envs/lighthouse36/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:632: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/mlv.py:68: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/mlv.py:250: The name tf.matrix_inverse is deprecated. Please use tf.linalg.inv instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wiss/lhao/junpeng/ws_lighthouse/lighthouse/geometry/projector.py:286: The name tf.cumprod is deprecated. Please use tf.math.cumprod instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up placeholders\n",
    "ref_image = tf.placeholder(dtype=tf.float32, shape=[None, height, width, 3])\n",
    "ref_depth = tf.placeholder(dtype=tf.float32, shape=[None, height, width])\n",
    "intrinsics = tf.placeholder(dtype=tf.float32, shape=[None, 3, 3])\n",
    "ref_pose = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4])\n",
    "src_images = tf.placeholder(dtype=tf.float32, shape=[None, height, width, 3])\n",
    "src_poses = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4, 1])\n",
    "env_pose = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4])\n",
    "\n",
    "# Set up model\n",
    "model = MLV()\n",
    "\n",
    "# We use the true depth bounds for testing\n",
    "# Adjust to estimated bounds for your dataset\n",
    "min_depth = tf.reduce_min(ref_depth)\n",
    "max_depth = tf.reduce_max(ref_depth)\n",
    "\n",
    "mpi_planes = pj.inv_depths(min_depth, max_depth, num_planes)\n",
    "\n",
    "pred = model.infer_mpi(src_images, ref_image, ref_pose, src_poses, intrinsics,\n",
    "                        mpi_planes)\n",
    "\n",
    "mpi_gt = model.img2mpi(ref_image,ref_depth,mpi_planes)\n",
    "\n",
    "lightvols, lightvol_centers, \\\n",
    "lightvol_side_lengths, \\\n",
    "cube_rel_shapes, \\\n",
    "cube_nest_inds = model.predict_lighting_vol(mpi_gt, mpi_planes,\n",
    "                                            intrinsics, cube_res,\n",
    "                                            scale_factors,\n",
    "                                            depth_clip=depth_clip)\n",
    "lightvols_out = nets.cube_net_multires(lightvols, cube_rel_shapes,\n",
    "                                        cube_nest_inds)\n",
    "output_envmap, all_shells_list = model.render_envmap(lightvols_out, lightvol_centers,\n",
    "                                        lightvol_side_lengths, cube_rel_shapes,\n",
    "                                        cube_nest_inds, ref_pose, env_pose,\n",
    "                                        theta_res, phi_res, r_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./lighthouse/model/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [12:01<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "id_ref=1\n",
    "id_src=3\n",
    "id_env=101\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "\n",
    "sess1 = tf.Session(config=config)\n",
    "\n",
    "with sess1 as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, os.path.join(checkpoint_dir, \"model.ckpt\"))\n",
    "\n",
    "    # i = 0\n",
    "    # for i in range(0, len(input_files)):\n",
    "    # print(\"running example:\", i)\n",
    "\n",
    "    # Load inputs\n",
    "    # batch = np.load(data_dir + input_files[i])\n",
    "    for id_env in tqdm(range(260)):\n",
    "        if id_ref == id_src:\n",
    "            continue\n",
    "        batch = get_mbatch(id_ref,id_src,id_env)\n",
    "    \n",
    "        output_lightvols_out_eval, output_lightvols_eval, output_envmap_eval, output_lightvol_centers_eval,all_shells_list_eval = sess.run(\n",
    "            [lightvols_out, lightvols, output_envmap, lightvol_centers,all_shells_list],\n",
    "            feed_dict={\n",
    "                ref_image: batch[\"ref_image\"],\n",
    "                ref_depth: batch[\"ref_depth\"],\n",
    "                intrinsics: batch[\"intrinsics\"],\n",
    "                ref_pose: batch[\"ref_pose\"],\n",
    "                src_images: batch[\"src_images\"],\n",
    "                src_poses: batch[\"src_poses\"],\n",
    "                env_pose: batch[\"env_pose\"]\n",
    "            })\n",
    "            \n",
    "        rgb = output_envmap_eval.squeeze()[:,:,[2,1,0]]\n",
    "        output_path = os.path.join(output_dir,\"env_ref{}_{}.pfm\".format(str(id_ref),str(id_env+1)))\n",
    "        # plt.imsave(output_path,rgb)\n",
    "        imageio.imwrite(output_path, rgb, format='PFM')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64, 64, 4)\n",
      "(1, 64, 64, 64, 4)\n",
      "[[0.        0.        0.6070476]]\n",
      "(1, 120, 240, 128, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output_lightvols_out_eval[0].shape) # (1, 64, 64, 64, 4)\n",
    "print(output_lightvols_eval[0].shape) # (1, 64, 64, 64, 4)\n",
    "print(output_lightvol_centers_eval[4])\n",
    "print(all_shells_list_eval[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('lighthouse36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cefdf0e84355fa00dca9834fb68422feeee2b75b244a6c4e9519d944e9722d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
